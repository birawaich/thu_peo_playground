{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 06\n",
    "\n",
    "Implement Value Iteration on MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "### DEFINITIONS\n",
    "\n",
    "class State:\n",
    "    \"\"\"A state (x and y coordinate)\"\"\"\n",
    "    \n",
    "    def __init__(self, x: int, y: int):\n",
    "        assert x < GRID_DIM_X and x >= 0, \"x out of bounds\"\n",
    "        assert y < GRID_DIM_Y and y >= 0, \"y out of bounds\"\n",
    "        self.coord_x = x\n",
    "        self.coord_y = y\n",
    "        return\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        # Check if 'other' is a state\n",
    "        if isinstance(other, State):\n",
    "            return self.coord_x == other.coord_x \\\n",
    "                and self.coord_y == other.coord_y\n",
    "        return False\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"State(x={self.coord_x}, y={self.coord_y})\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "class Action:\n",
    "    \"\"\"an action, simply encoded as an integer\"\"\"\n",
    "    # 0 ... stay\n",
    "    # 1 ... up\n",
    "    # 2 ... right\n",
    "    # 3 ... down\n",
    "    # 4 ... left\n",
    "\n",
    "    def __init__(self,action: int):\n",
    "        assert action < 5 and action >= 0, \"action out of bounds\"\n",
    "        self.action = action\n",
    "        return\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        # Check if 'other' is a state\n",
    "        if isinstance(other, Action):\n",
    "            return self.action == other.action\n",
    "        return False\n",
    "    \n",
    "class Policy:\n",
    "    \"\"\"Class to represent a policy\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._action = np.zeros((GRID_DIM_X,GRID_DIM_Y)) #access as value_function[state.coord_x, state.coord_y]\n",
    "        return\n",
    "\n",
    "    def set_value(self, state: State, action: Action):\n",
    "        self._action[state.coord_x, state.coord_y] = action.action\n",
    "        return\n",
    "    def get_value(self, state: State) -> Action:\n",
    "        return Action(self._action[state.coord_x, state.coord_y])\n",
    "    \n",
    "    def print_arrow_table(self):\n",
    "        \"\"\"print the policy as ASCII art, generated by ChatGPT\"\"\"\n",
    "        print(\"Policy\")\n",
    "\n",
    "        matrix = self._action\n",
    "        # Define the symbols for each value\n",
    "        symbols = {\n",
    "            0: \"•\",      # stay\n",
    "            1: \"↑\",      # up\n",
    "            2: \"→\",      # right\n",
    "            3: \"↓\",      # down\n",
    "            4: \"←\"       # left\n",
    "        }\n",
    "\n",
    "        # Determine the width of each cell for alignment\n",
    "        cell_width = max(len(symbols[v]) for row in matrix for v in row) + 2\n",
    "\n",
    "        # Print the table with ASCII box drawing characters\n",
    "        top_border = \"┌\" + \"┬\".join(\"─\" * cell_width for _ in matrix[0]) + \"┐\"\n",
    "        row_separator = \"├\" + \"┼\".join(\"─\" * cell_width for _ in matrix[0]) + \"┤\"\n",
    "        bottom_border = \"└\" + \"┴\".join(\"─\" * cell_width for _ in matrix[0]) + \"┘\"\n",
    "\n",
    "        # Print top border\n",
    "        print(top_border)\n",
    "        \n",
    "        for i, row in enumerate(matrix):\n",
    "            # Print row with arrows\n",
    "            print(\"│\" + \"│\".join(f\"{symbols[val]:^{cell_width}}\" for val in row) + \"│\")\n",
    "            \n",
    "            # Print row separator, except after the last row\n",
    "            if i < len(matrix) - 1:\n",
    "                print(row_separator)\n",
    "        \n",
    "        # Print bottom border\n",
    "        print(bottom_border)\n",
    "    \n",
    "class ValueFunction:\n",
    "    \"\"\"Value Function\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._storage = np.zeros((GRID_DIM_X,GRID_DIM_Y)) #access as value_function[state.coord_x, state.coord_y]\n",
    "\n",
    "    def get_value(self, state: State) -> float:\n",
    "        return self._storage[state.coord_x, state.coord_y]\n",
    "    def set_value(self, state: State, value: float):\n",
    "        self._storage[state.coord_x, state.coord_y] = value\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"returns a figure and axes to plot the value function as a heat map\"\"\"\n",
    "        fig, ax = plt.subplots()\n",
    "        cax = ax.imshow(self._storage, cmap='viridis', interpolation='nearest')\n",
    "        fig.colorbar(cax, ax=ax, label=\"Value\")\n",
    "        ax.set_title(\"Value Function Heatmap\")\n",
    "        return fig, ax\n",
    "    \n",
    "    def extract_policy(self,\n",
    "                    set_states: List[State],\n",
    "                    set_actions: List[Action]) -> Policy:\n",
    "        policy = Policy()\n",
    "        value_function_discard = ValueFunction()\n",
    "        for state in set_states:\n",
    "            # just greedily take actin that gives the highest value\n",
    "            action, val = step_vi(state_evaluate=state,\n",
    "                                  set_states=set_states,\n",
    "                                  set_actions=set_actions,\n",
    "                                  value_function_new=value_function_discard,\n",
    "                                  value_function_old=self)\n",
    "            policy.set_value(state,action)\n",
    "\n",
    "        return policy\n",
    "    \n",
    "    def extract_qfunction(self,\n",
    "                          set_states: List[State],\n",
    "                          set_actions: List[Action]) -> np.ndarray:\n",
    "        \"\"\"extracts a qfunction as a matrix [s.x,s.y,a]\"\"\"\n",
    "        qmatrix = np.ndarray((GRID_DIM_X,GRID_DIM_Y,5))\n",
    "        for state in set_states:\n",
    "            for action in set_actions:\n",
    "                val = substep_vi(state_evaluate=state,\n",
    "                                         action=action,\n",
    "                                         set_states=set_states,\n",
    "                                         value_function_old=self)\n",
    "                qmatrix[int(state.coord_x),int(state.coord_y),int(action.action)] = val\n",
    "        return qmatrix\n",
    "\n",
    "def print_qfunction(qmatrix: np.ndarray):\n",
    "    \"\"\"print a qfunction that is represented as a matrix\n",
    "    \n",
    "    prettied up with ChatGPT\"\"\"\n",
    "    assert qmatrix.shape == (GRID_DIM_X,GRID_DIM_Y,5)\\\n",
    "        , \"Dude, check what you feed your functions!\"\n",
    "\n",
    "    # Mapping of actions to symbols\n",
    "    action_symbols = {\n",
    "        0: \"•\",  # stay\n",
    "        1: \"↑\",  # up\n",
    "        2: \"→\",  # right\n",
    "        3: \"↓\",  # down\n",
    "        4: \"←\"   # left\n",
    "    }\n",
    "\n",
    "    print(\"Q Function Table\")\n",
    "    \n",
    "    # Print the header\n",
    "    print(f\"{'X':<5} {'Y':<5} {'Action':<10} {'Q-Value':<10}\")\n",
    "    print(\"-\" * 30)  # Separator line\n",
    "\n",
    "    # Iterate through each coordinate and action\n",
    "    for x in range(GRID_DIM_X):\n",
    "        for y in range(GRID_DIM_Y):\n",
    "            for a in range(5):\n",
    "                # Print each row in the table format with action symbols\n",
    "                print(f\"{x:<5} {y:<5} {action_symbols[a]:<10} {qmatrix[x, y, a]:<10.4f}\")\n",
    "\n",
    "def reward_funciton(state: State):\n",
    "    \"\"\"reward for being at a certain state\"\"\"\n",
    "    if(TARGET_STATE == state):\n",
    "        return 10\n",
    "    return -1\n",
    "\n",
    "def transition_probability(state_src: State, state_dst: State, action: Action):\n",
    "    \"\"\"if being in state src and applying action, what is the probability to end up at state dst\n",
    "    \n",
    "    note that illegal states do not need to be checked, as dst needs to be valid state!\"\"\"\n",
    "    # action = stay\n",
    "    if state_dst == state_src\\\n",
    "        and action.action == 0:\n",
    "        return 1\n",
    "    # action is not stay, but still stay\n",
    "    if state_dst == state_src\\\n",
    "        and action.action != 0:\n",
    "        return 1 - TRANSITION_PROB_P\n",
    "    # desired actions\n",
    "    if (state_dst.coord_x == state_src.coord_x\\\n",
    "        and state_dst.coord_y == state_src.coord_y+1\\\n",
    "        and action.action == 2)\\\n",
    "        or\\\n",
    "        (state_dst.coord_x == state_src.coord_x\\\n",
    "        and state_dst.coord_y == state_src.coord_y-1\\\n",
    "        and action.action == 4)\\\n",
    "        or\\\n",
    "        (state_dst.coord_x == state_src.coord_x-1\\\n",
    "        and state_dst.coord_y == state_src.coord_y\\\n",
    "        and action.action == 1)\\\n",
    "        or\\\n",
    "        (state_dst.coord_x == state_src.coord_x+1\\\n",
    "        and state_dst.coord_y == state_src.coord_y\\\n",
    "        and action.action == 3):\n",
    "        return TRANSITION_PROB_P\n",
    "    # everything else\n",
    "    return 0\n",
    "\n",
    "def substep_vi(action: Action,\n",
    "               state_evaluate: State,\n",
    "               set_states: List[State],\n",
    "               value_function_old: ValueFunction) -> float:\n",
    "    \"\"\"Substep in VI Step: Calculate the value for an action\n",
    "\n",
    "    (extracted as substep to calculate Q function efficiently)\"\"\"\n",
    "    sum_nextstep = 0\n",
    "    for state_next in set_states:\n",
    "        sum_nextstep += transition_probability(state_src=state_evaluate,\n",
    "                                                state_dst=state_next,\n",
    "                                                action=action)\\\n",
    "                        *value_function_old.get_value(state_next)\n",
    "    val_action = reward_funciton(state_evaluate)\\\n",
    "        + DISCOUNT*sum_nextstep\n",
    "    return val_action\n",
    "\n",
    "def step_vi(state_evaluate: State #where evaluating\n",
    "            , set_states: List[State] #set of all states\n",
    "            , set_actions: List[Action] #set of all actions\n",
    "            , value_function_old: ValueFunction #old value function\n",
    "            , value_function_new: ValueFunction) -> Tuple[Action, float]: #new value function\n",
    "    \"\"\"the maximization step with a deterministic policy in VI\n",
    "    \n",
    "    returns the action and the step that led to that maximum\"\"\"\n",
    "    maximum = -np.inf\n",
    "    action_maximum = None\n",
    "    for action in set_actions:\n",
    "        val_action = substep_vi(action=action\n",
    "                                ,state_evaluate=state_evaluate\n",
    "                                ,set_states=set_states\n",
    "                                ,value_function_old=value_function_old)\n",
    "        if val_action > maximum:\n",
    "            maximum = val_action\n",
    "            action_maximum = action\n",
    "    # print(f\"Updating {state_evaluate} from {value_function_old.get_value(state_evaluate)} -> {maximum}\")\n",
    "    value_function_new.set_value(state_evaluate, maximum)\n",
    "\n",
    "    return (action_maximum ,maximum)\n",
    "\n",
    "def value_iteration(value_function_init: ValueFunction, #initial value function\n",
    "                    set_states: List[State], #all possible states\n",
    "                    set_actions: List[Action], #all possible actions\n",
    "                    epsilon: float = 0.42 #when stopping to converge\n",
    "                    ) -> ValueFunction:\n",
    "    \"\"\"runs VI and returns a (new) value function\n",
    "    \n",
    "    the algorithm terminates if the change in the infinty norm of two consequitive value\n",
    "    functions is < epsilon\"\"\"\n",
    "    step = 0\n",
    "    value_function_old = value_function_init\n",
    "    value_function_new = copy.deepcopy(value_function_init)\n",
    "    while True:\n",
    "        norm = -np.inf\n",
    "        for state in set_states:\n",
    "            step_vi(state_evaluate=state,\n",
    "                    set_states=set_states,set_actions=set_actions,\n",
    "                    value_function_old=value_function_old,\n",
    "                    value_function_new=value_function_new)\n",
    "            #update norm\n",
    "            norm_state = np.abs(value_function_new.get_value(state)\n",
    "                                - value_function_old.get_value(state))\n",
    "            if norm_state > norm:\n",
    "                norm = norm_state\n",
    "        # update value funciton\n",
    "        value_function_old = copy.deepcopy(value_function_new)\n",
    "        # check break criterion\n",
    "        print(f\"[{step}]\\tLIninity Norm: {norm}\")\n",
    "        if norm < epsilon:\n",
    "            break\n",
    "        # update step\n",
    "        step += 1\n",
    "    # return found value function\n",
    "    print(f\"Value Interation converged after {step} steps.\")\n",
    "    return value_function_old          \n",
    "            \n",
    "\n",
    "\n",
    "### DEFINITIONS\n",
    "\n",
    "GRID_DIM_X = 4\n",
    "GRID_DIM_Y = 4\n",
    "\n",
    "TARGET_STATE = State(x=3,y=3)\n",
    "\n",
    "DISCOUNT = 0.95\n",
    "\n",
    "TRANSITION_PROB_P = 0.7 #probability that the transition actually works out\n",
    "\n",
    "### SCRIPT\n",
    "\n",
    "set_states = [] #set of all states\n",
    "for x in range(0,GRID_DIM_X):\n",
    "    for y in range(0,GRID_DIM_Y):\n",
    "        set_states.append(State(x=x,y=y))\n",
    "set_actions = []\n",
    "for a in range(0,5):\n",
    "    set_actions.append(Action(a))\n",
    "\n",
    "value_function = ValueFunction()\n",
    "\n",
    "# run VI\n",
    "found_value_function = value_iteration(value_function_init=value_function\n",
    "                            ,set_states=set_states\n",
    "                            ,set_actions=set_actions\n",
    "                            ,epsilon=0.00001337)\n",
    "\n",
    "# plot Value Function\n",
    "fig, ax = found_value_function.plot()\n",
    "plt.show()\n",
    "fig.savefig(\"./out/figure_hw06.png\",format='png', dpi=300)\n",
    "\n",
    "# extract policy\n",
    "optimal_policy = found_value_function.extract_policy(\n",
    "    set_states=set_states, set_actions=set_actions)\n",
    "\n",
    "# print policy\n",
    "optimal_policy.print_arrow_table()\n",
    "\n",
    "# qfunction\n",
    "qfunction = found_value_function.extract_qfunction(\n",
    "    set_actions=set_actions, set_states=set_states)\n",
    "print_qfunction(qfunction)\n",
    "\n",
    "print(\"DONE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thu_peo_playground",
   "language": "python",
   "name": "thu_peo_playground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
