{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 06\n",
    "\n",
    "Implement Value Iteration on MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINITIONS\n",
    "\n",
    "class State:\n",
    "    \"\"\"A state (x and y coordinate)\"\"\"\n",
    "    \n",
    "    def __init__(self, x: int, y: int):\n",
    "        assert x < GRID_DIM_X and x >= 0, \"x out of bounds\"\n",
    "        assert y < GRID_DIM_Y and y >= 0, \"y out of bounds\"\n",
    "        self.coord_x = x\n",
    "        self.coord_y = y\n",
    "        return\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        # Check if 'other' is a state\n",
    "        if isinstance(other, State):\n",
    "            return self.coord_x == other.coord_x \\\n",
    "                and self.coord_y == other.coord_y\n",
    "        return False\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"State(x={self.coord_x}, y={self.coord_y})\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "class Action:\n",
    "    \"\"\"an action, simply encoded as an integer\"\"\"\n",
    "    # 0 ... stay\n",
    "    # 1 ... up\n",
    "    # 2 ... right\n",
    "    # 3 ... down\n",
    "    # 4 ... left\n",
    "\n",
    "    def __init__(self,action: int):\n",
    "        assert action < 5 and action >= 0, \"action out of bounds\"\n",
    "        self.action = action\n",
    "        return\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        # Check if 'other' is a state\n",
    "        if isinstance(other, Action):\n",
    "            return self.action == other.action\n",
    "        return False\n",
    "    \n",
    "class ValueFunction:\n",
    "    \"\"\"Value Function\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._storage = np.zeros((GRID_DIM_X,GRID_DIM_Y)) #access as value_function[state.coord_x, state.coord_y]\n",
    "\n",
    "    def get_value(self, state: State) -> float:\n",
    "        return self._storage[state.coord_x, state.coord_y]\n",
    "    def set_value(self, state: State, value: float):\n",
    "        self._storage[state.coord_x, state.coord_y] = value\n",
    "    \n",
    "def reward_funciton(state: State):\n",
    "    \"\"\"reward for being at a certain state\"\"\"\n",
    "    if(TARGET_STATE == state):\n",
    "        return 10\n",
    "    return -1\n",
    "\n",
    "def transition_probability(state_src: State, state_dst: State, action: Action):\n",
    "    \"\"\"if being in state src and applying action, what is the probability to end up at state dst\n",
    "    \n",
    "    note that illegal states do not need to be checked, as dst needs to be valid state!\"\"\"\n",
    "    # action = stay\n",
    "    if state_dst == state_src\\\n",
    "        and action.action == 0:\n",
    "        return 1\n",
    "    # action is not stay, but still stay\n",
    "    if state_dst == state_src\\\n",
    "        and action.action != 0:\n",
    "        return 1 - TRANSITION_PROB_P\n",
    "    # desired actions\n",
    "    if (state_dst.coord_x == state_src.coord_x\\\n",
    "        and state_dst.coord_y == state_src.coord_y+1\\\n",
    "        and action.action == 1)\\\n",
    "        or\\\n",
    "        (state_dst.coord_x == state_src.coord_x\\\n",
    "        and state_dst.coord_y == state_src.coord_y-1\\\n",
    "        and action.action == 3)\\\n",
    "        or\\\n",
    "        (state_dst.coord_x == state_src.coord_x-1\\\n",
    "        and state_dst.coord_y == state_src.coord_y\\\n",
    "        and action.action == 4)\\\n",
    "        or\\\n",
    "        (state_dst.coord_x == state_src.coord_x+1\\\n",
    "        and state_dst.coord_y == state_src.coord_y\\\n",
    "        and action.action == 2):\n",
    "        return TRANSITION_PROB_P\n",
    "    # everything else\n",
    "    return 0\n",
    "\n",
    "def step_vi(state_evaluate: State #where evaluating\n",
    "            , set_states: List[State] #set of all states\n",
    "            , set_actions: List[Action] #set of all actions\n",
    "            , value_function_old: ValueFunction #old value function\n",
    "            , value_function_new: ValueFunction): #new value function\n",
    "    \"\"\"the maximization step with a deterministic policy in VI\"\"\"\n",
    "    maximum = -np.inf\n",
    "    for action in set_actions:\n",
    "        sum_nextstep = 0\n",
    "        for state_next in set_states:\n",
    "            sum_nextstep += transition_probability(state_src=state_evaluate,\n",
    "                                                   state_dst=state_next,\n",
    "                                                   action=action)\\\n",
    "                            *value_function_old.get_value(state_next)\n",
    "        val_action = reward_funciton(state_evaluate)\\\n",
    "            + DISCOUNT*sum_nextstep\n",
    "        if val_action > maximum:\n",
    "            maximum = val_action\n",
    "    # print(f\"Updating {state_evaluate} from {value_function_old.get_value(state_evaluate)} -> {maximum}\")\n",
    "    value_function_new.set_value(state_evaluate, maximum)\n",
    "\n",
    "def value_iteration(value_function_init: ValueFunction, #initial value function\n",
    "                    set_states: List[State], #all possible states\n",
    "                    set_actions: List[Action], #all possible actions\n",
    "                    epsilon: float = 0.42 #when stopping to converge\n",
    "                    ) -> ValueFunction:\n",
    "    \"\"\"runs VI and returns a (new) value function\n",
    "    \n",
    "    the algorithm terminates if the change in the infinty norm of two consequitive value\n",
    "    functions is < epsilon\"\"\"\n",
    "    step = 0\n",
    "    value_function_old = value_function_init\n",
    "    value_function_new = copy.deepcopy(value_function_init)\n",
    "    while True:\n",
    "        norm = -np.inf\n",
    "        for state in set_states:\n",
    "            step_vi(state_evaluate=state,\n",
    "                    set_states=set_states,set_actions=set_actions,\n",
    "                    value_function_old=value_function_old,\n",
    "                    value_function_new=value_function_new)\n",
    "            #update norm\n",
    "            norm_state = np.abs(value_function_new.get_value(state)\n",
    "                                - value_function_old.get_value(state))\n",
    "            if norm_state > norm:\n",
    "                norm = norm_state\n",
    "        # update value funciton\n",
    "        value_function_old = copy.deepcopy(value_function_new)\n",
    "        # check break criterion\n",
    "        print(f\"[{step}]\\tLIninity Norm: {norm}\")\n",
    "        if norm < epsilon:\n",
    "            break\n",
    "        # update step\n",
    "        step += 1\n",
    "    # return found value function\n",
    "    return value_function_old          \n",
    "            \n",
    "\n",
    "\n",
    "### DEFINITIONS\n",
    "\n",
    "GRID_DIM_X = 4\n",
    "GRID_DIM_Y = 4\n",
    "\n",
    "TARGET_STATE = State(x=3,y=3)\n",
    "\n",
    "DISCOUNT = 0.95\n",
    "\n",
    "TRANSITION_PROB_P = 0.7 #probability that the transition actually works out\n",
    "\n",
    "### SCRIPT\n",
    "\n",
    "set_states = [] #set of all states\n",
    "for x in range(0,GRID_DIM_X):\n",
    "    for y in range(0,GRID_DIM_Y):\n",
    "        set_states.append(State(x=x,y=y))\n",
    "set_actions = []\n",
    "for a in range(0,5):\n",
    "    set_actions.append(Action(a))\n",
    "\n",
    "value_function = ValueFunction()\n",
    "\n",
    "value_iteration(value_function_init=value_function\n",
    "                ,set_states=set_states\n",
    "                ,set_actions=set_actions\n",
    "                ,epsilon=0.5)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thu_peo_playground",
   "language": "python",
   "name": "thu_peo_playground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
